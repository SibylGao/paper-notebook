# paper-notebook
记录这段时间的论文，MVS相关

2020.7.20：
昨天看了一下patch match的算法，感觉就是随机一个patch中心位置然后选择一个最接近的offset进行衰减半径的搜索。
今天看了patch match stero这篇的论文，感觉有点难理解，它主要是为了解决匹配区域内视差不一致的问题，由于深度不一样会导致视差不一样，这篇文章将patch match用于MVS方法中，对每一个像素都搜索了一个最优平面，即最优的视差，用点法式来表示最优的平面（点坐标+法向量），上次讲论文小格也提到了迭代的问题，在这篇文章中是分别对图像从左上角到右下角，或者右下角到左上角进行遍历，用的也是patch match中随机+搜索更新的规则，并且定义了四个更新的规则，有一点不太懂的地方是不知道他计算代价里面的一对匹配点p，q是分别是左图和右图的一组点，还是同一幅图中位于同一3D平面的点，这个算法属于局部匹配算法，并且由于对像素都是进行了好几次的遍历，所以速度会比较慢

2020.7.21：
Massively Parallel Multiview Stereopsis by Surface Normal Diffusion这篇论文主要为了解决patch match stereo 里面迭代传播比较慢的问题，原始的patch match是从左上到右下或者反过来，这样需要遍历6遍全图，这篇文章是将像素格分为红格和黑格，并且是并行的处理这两类的格子，在处理黑格的时候，所有黑格可以并行处理，因为他们法向量和深度的传播是靠周围的红格，相邻的黑格更新平面不会互相影响，这样的方法非常新颖也大大提高了更新的速度，并且还可以通过调整周围红格的搜索范围来提高速度。作者还将这种方法推广到了多张图像，具体就是用单应性矩阵来完成不同平面坐标系之间的变换，将patch match中左右图之间的cost function推广到所有与参考图像有重合的图像，并且将所有的视图的cost进行一个排序，取代价值最低的几个cost进行累加，作为更新最优平面时判断的依据，分别将每个视图作为参考图像重复上面的步骤，就能分别计算每个视图的深度图，最后将所有深度图进行融合，并且也像patch match一样最后要做一个左右一致性检查，不满足的点将被删除。这种并行处理的思路大大提高了patch match的速度，作者提到它的速度和准确率都比PMVS要高。

2020.7.22
MVSNet: Depth Inference for Unstructured Multi-view Stereo
这篇文章跟之前看过的端到端重建深度的框架很像，都是提取特征---warp构建feature volume---构建cost volume---输出深度图---refine这样的一个结构。并且作者在构建feature volume这一步划分256个深度平面（跟我之前讲的那篇深度学习+平面扫描的文章很像），分别计算对应的平面坐标转换的单应性矩阵，然后再将其它图像warp到参考图像的坐标系下，根据方差来算cost volume，再通过U-Net结构来生成probability volume，计算深度概率图的数学期望作为估计的深度图，最后将初步估计的深度图和参考图像经过神经网络求出一个残差，再加到初始的深度图上得到refine之后的深度图，最后作者还根据深度概率图过滤掉一些不可靠的深度点，至此完成了深度图的估计。作者再DTU数据集上做了测试，跟patch match的Gipuma算法做了一个对比，虽然MVSNet的准确率是要低于Gipuma的，但是完整度要高于Gipuma，并且速度也更快。

2020.7.23
Recurrent MVSNet for High-resolution Multi-view Stereo Depth Inference
这篇论文是在MVSNet基础之上的改进，MVSNet是使用U-Net来计算probability volume，这样的方式对内存的消耗十分大，因此作者提出了使用循环神经网络的一种GRU来对cost volume进行正则化，关于GRU我去查了一些资料，感觉跟LSTM很像都是对数据有选择性的遗忘和记忆，作者将cost volume根据深度的不同拆分成多个input volume，作为一个序列输入GRU中，用循环神经网络的结构解决输入序列之间的相关性，首先通过一个2D卷积层将32-channel转化为16-channel作为GRU层的输入。之后，每一层GRU输出作为下一层输入，最后一层的GRU输出为1，再通过一个softmax层来输出深度概率，用这种方法代替了MVSNet中的U-Net以及残差结构。最后作者实验出来的结果是准确率虽然仍然不如Gipuma，但是完整性和准确率的都要高于MVSNet，对内存的占用也比3D的CNN更少。

2020.7.24
Point-MVSNet
这篇文章在前面部分是和MVSNet一致的，用MVSNet来提取出粗略的深度信息，在MVSNet和Recurrent MVSNet中到了这一步之后都会有一个refine的步骤，MVSNet是将彩色图像+深度图生成一个残差来得到更好的结果，Recurrent MVSNet是使用GRU来代替后面的这些步骤，在之前看的论文对比中可以发现MVSNet系列的方法对深度估计的完整度并不高，因此作者这篇论文将2D和3D特征进行融合以求得到更加完整的深度结果。这篇文章最核心的地方就是pointflow这个结构，pointflow首先是将2D的深度图“unproject”成3D的点云，但是我有去查过DGCNN生成点云的方法，里面说的是从通用的高斯模型中进行采样，再通过K近邻来聚合（？）这里细节的地方没有来得及看，准备明天再去仔细看一下DGCNN的方法，里面还用到了边卷积，感觉是将概率作为每一条边的权重然后用求质心的方法来计算出每个unproject point的偏移，最后得到参考图像的残差值，对MVSNet输出的深度图进行加强。作者再使用MVSNet的时候，作者将特征图下采样到了原图的1/8，因此再进行3D卷积的时候占用的内存更少，但这样初始生成的深度图应该会比MVSNet更稀疏。这篇文章最后的结果是完整性和准确率都要高于MVSNet，并且内存占用更少。

2020.7.30
Dynamic Graph CNN for Learning on Point Clouds
这篇文章是用edgeconv的方法来对输入的3D点云进行聚类和分割，是基于PointNet的一个工作，整个网络分为classification和segmentation两个部分，classification的网络输出一个1D的用于分类的描述子，然后segmentation网络将这个描述子和每一层edgeconv输出的特征综合起来计算出每个点的classification score。对于每个edgeconv模块共享边缘函数，并且这个计算特征的边缘函数是用MLP来表示的，也就是一个多层感知机结构，在一个set中点与点之间是全连接的一个关系。总体上来说，我感觉是将KNN的聚合和分类过程中用到的特征空间中的距离用这个边缘函数来代替，并且不断地更新边缘函数来动态地更新每一个set中所含的点，因此也有了动态图这个概念。edgeconv就是一个多层感知机结构（？但是为什么专门提出来是edgeconv，跟多层感知机有什么区别？这里似乎是每一个神经元之间的连线代表两个点之间连接的边），此外网络中还有一个超参数K，跟KNN中的K相对应。还有一个对应的Point cloud transform block，这个模块是为了将一个输入的set变换到标准的三维空间（？边卷积想象起来还是有一点太抽象，但是整个网络的输入应该是标准三维空间的一个点云）一个edgeconv模块如果是a层的一个MLP，经过池化之后，输出的特征也是a维的，如果有n个点，那输出张量大小就是n*a。作者分别实验了分类任务和分割任务，在分类任务中，表现是要优于PointNet系列的结果的，并且参数量也比pointnet更少；在分割任务上也优于了PointNet。看完了DGCNN感觉还是不太理解PointMVS里面的unproject点以及它是如何计算偏移和图像残差的，DGCNN感觉更像是一个分割算法，本身并不会对点云的位置进行矫正。

2020.7.31
Polarimetric Multi-View Inverse Rendering
这篇文章中作者提出了使用偏振光相机图像作为输入来对点云进行一个refine让三维重建的结果更加的精细。之前看到的很多refine的方法都是基于彩色图像的photometric，还是第一次看见用偏振光图像来做的。整个算法的输入是一系列的彩色图像以及对应的偏振光图像，先根据彩色图像序列使用SFM的方法解算出相机的姿态，再根据解算出来的相机姿态使用MVS系列算法得到一个初步的点云，最后用彩色图像+偏振光图像（Photometric and polarimetric optimization）的方法来对点云进行一个refine得到更精细的结果，在SFM和MVS算法上面作者选择的是COLMAP+OpenMVS，作者着重介绍了一下photometric+polarimetric优化的这一部分。优化的参数分别是三角网格的顶点，每个顶点的albedo（albedo不是相对于一个平面的吗？）以及场景的光照；cost function包括了四项，分别是Photometric rendering term（这一项用来表示实际的光强和用参数估计的光照模型下三角网格顶点光强的差值），Polarimetric term（这一部分函数很复杂，但感觉是衡量根据投影计算出来的法向量跟偏振光估计出来的法向量的差距，并且同时消除了pi和pi/2 ambiguities），Geometric smoothness term（这一项是通过约束三角网格极其周围的网格法向量来控制几何形状的平滑），Photometric smoothness term（这一项是约束相邻三角网格的patch之间的albedo差异），用数值方法优化这个函数，就能得到refine之后精细的三维模型。通过实验可以看出本文的方法比其他三维重建的方法得到的结果都要精细，在准确率和完整度上也高于了CMPMVS和OpenMVS，这证明了使用偏振光的方法对于提升三维重建的精度是十分有效的。


2020.8.3
Planar Prior Assisted PatchMatch Multi-View Stereo
这篇文章作者主要旨在解决patch match算法里面用基于光度一致性的cost function对低纹理区域的估计不准确的问题，因此提出了一种基于平面先验的patch match算法。整个的算法流程为，先用基于光度一致性的patch match方法中的cost function（例如COLMAP,ACMH）来初步得到一个重建的结果，再根据这个结果生成三角网格，同一个网格平面中的点法向量相同，再将三角网格平面的法向量作为先验推导出一个新的cost function，这个cost function有两项，一项是基于光度一致性假设，另一项是三角网格平面的假设，并且这个先验包含了三项（分别是三角网格平面距离之间的差值为指数，三角网格法向量夹角为指数，和一个常数项，这个公式是作者自己定义的，感觉是为了衡量假设与先验之间的区别，有一点点意会，但是不知道为什么要这样设）；并且在进行光度一致性假设估计的时候，cost function里面有一组权重系数，为了表示patch l 周围的neighbor patch的可见性，也将其作为概率的先验，并且是通过蒙特卡洛采样的方式来统计这个概率，作为权重（应该就是直接采样然后通过频数计算频率以频率来代替概率，说实话这一步我不知道为什么要加这一个权重和先验）。本文中更新的方法也是使用了棋盘并行更新的方法，但是它在更新的最后都会有一步refine（？这一步也没有看懂），总的来说感觉整个方法还是基于patch match的框架，只是用了大量统计学的方法来对cost function和propagation进行了优化，最后作者在ETH3D数据集上的结果准确率没有COLMAP高，但是F1 score要高于现有的算法，作者也提到了它的性能跟ACMM相当。

2020.8.5-8.6
Confidence-Based Large-Scale Dense Multi-View Stereo
 整个算法特别复杂工作量也特别多（这就是期刊和会议的不同吗...）整个算法的流程可以概括为：baseline Patchmatch，根据初始生成的深度生成法向量，基于神经网络的confidence prediction，根据置信度来进行插值，（对于outdoor场景下）用mask和检查法向量和光线的夹角来去掉天空的深度值，最后再使用patch match的方法+先验来进行refine，最后再对生成的各个视点的图进行fusion。在baseline patchmatch这一步，首先是使用了仿射系数omega对光强进行了一个加权，再求一个q和p之间的correlation计算，这里的q和p的光强是指实际光强减去加权光强（这一步不知道为什么要这样做），最后再除以一个表示模的量，因此是正则化之后的cost即ANCC。为了增强鲁棒性，取了前Kb个cost值来计算，这一步使用棋盘格的propagation方法。
计算法向量这一步，取的是像素p上下左右四个像素做逆投影变换之后的差值，取这两个向量的叉乘方向作为p的法向量。在confidence prediction这一步里面，为了得到连续的confidence的值使用了神经网络的方式来得到一个confidence值，为了生成正负样本，作者定义了一个算法来评价样本是可靠还是不可靠（这个算法中的参数是作者自己手动设置的）判断一个点的深度是否可靠有两个条件分别是geometric consistency（p点逆投影到neighbor img q的深度值和q点附近四个像素插值得到的深度之间的差距）和spatial consistency（source img逆投影到reference img上的投影距离和法向量夹角的误差，使用投票机制）满足spatial consistency函数的投票值大于一个阈值，以及geometric consistency函数小于一个阈值被认为是正样本，否则是负样本，将其输入到一个神经网络，计算出置信值。然后就是基于confidence值进行插值，这一步是基于weight least base方法，这一部分实在是没有看懂，后面再去查一下WLB-based方法，但是看能量函数是两项分别是深度置信，以及彩色图像和深度图的二次导数（应该是在置信度低的区域由彩色图像和深度图像中导数变化比较大的区域传递到导数低的区域？）。最后是refinement和fusion，在上一步中得到的插值结果是作为这一步的先验，感觉跟上一篇看的论文Planar Prior Assisted PatchMatch Multi-View Stereo这里的方法很像，这里就不再赘述了。（但是整个算法中用了两次patch match感觉很奇怪，最后花费的时间肯定也会很多）最后作者在ETH3D数据集上的结果，优于了PMVS,OpenMVS等算法，在ETH3D和DTU数据集上都能达到state-of-the-art，但代价是会花费很长的时间。

